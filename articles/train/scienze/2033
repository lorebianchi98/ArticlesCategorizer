Intelligenza artificiale, è possibile imporre limiti morali agli algoritmi? Forse è stata trovata una soluzione
Algoritmi razzisti o sessisti? Forse riflettono solo il mondo che noi abbiamo “programmato”
Dall’equità sociale alla medicina
Si chiamano algoritmi Seldonian, sono ispirati dal ciclo della Fondazione di Asimov e garantirebbero l’equità delle intelligenze artificiali
L’Intelligenza artificiale trova sempre più applicazioni, negli ambiti più disparati: dallo sviluppo di nuove protesi alla produzione agricola. Non mancano le preoccupazioni – anche in ambito europeo – per quanto riguarda diversi problemi di convivenza tra umani e macchine. 
L’intelligenza artificiale ci renderà disoccupati oppure ci sarà una fase intermedia in cui saremo sempre più impiegati in lavori intellettuali? Intanto, si avverte già il problema – definito da alcuni osservatori – l’aspetto “psicopatico” degli algoritmi, i quali potrebbero risultare politicamente scorretti di gran lunga più di noi. 
Questo ha un impatto non banale quando scegliamo di affidare all’Ia a quale corriere assegnare una consegna o quale candidato scegliere per un impiego lavorativo. 
Il 10 maggio scorso sono stati discussi alla National science foundation (Nsf) diversi problemi etici legati allo sviluppo di una Ia più equa. Un recente studio pubblicato su Science riprende l’argomento.  
Oggi affidare solo agli algoritmi la previsione dei livelli di criminalità o valutare il credito di una persona significa penalizzare gli individui su base etnica e di genere. Perché le statistiche sono il risultato di un background fatto di pregiudizi e di una lenta risposta della società a farvi fronte. 
Ma i dati statistici sono anche ciò di cui si avvalgono le Ia, le quali si trovano a esserne contagiate. Così la giovane madre o il nero del quartiere povero potrebbero essere svantaggiati in partenza. I ricercatori dell’università del Massachusetts sembrano avere individuato una soluzione al problema.
Il focus è quello di controllare in che modo un algoritmo di machine lerning, ovvero in grado di imparare, possa evolversi nel suo operato, senza causare i danni appena accennati. Lo studio prende in esame i cosiddetti «algoritmi Seldonian» (da un personaggio del ciclo della Fondazione di Isaac Asimov). 
Nell’esperimento condotto nella ricerca è stato realizzato un algoritmo in grado di prevedere il rendimento degli studenti, con degli accorgimenti per evitare una distorsione di genere nella valutazione. In sostanza anche delle semplici modifiche nella progettazione iniziale possono dare risultati notevoli. 
Forse le macchine non avranno mai una morale, non concepiranno mai l’etica, ma così come siamo riusciti nostro malgrado a “contagiarli” con i nostri pregiudizi allo stesso modo possiamo e dobbiamo stare più attenti  nel prevenire o evitare queste conseguenze. 
Questo è anche sintomo del fatto che l’Ia difficilmente potrà un giorno sostituire il contributo umano in tutti gli ambiti lavorativi.
Gli algoritmi Seldonian si baseranno su un recente documento in cui sono state elencate 21 diverse definizioni di equità. 
«È importante consentire all’utente di selezionare la definizione appropriata per l’applicazione prevista – spiega uno dei principali firmatari dello studio, Philip Thomas – L’interfaccia fornita con un algoritmo di Seldonian consente all’utente di fare proprio questo: definire cosa significa “comportamento indesiderato” per la propria applicazione».
In questo modo l’algoritmo è stato messo in pratica nella previsione dei voti medi ottenuti in un gruppo di 43mila studenti brasiliani. Il programma era in grado di specificare agli utenti quali vincoli applicare nell’operato, ottenendo risultati promettenti.
Questo approccio non si limita solo a garantire l’equità dell’algoritmo, può essere applicato infatti anche in campo medico, permettendo al programma di “comprendere” tempestivamente le situazioni di emergenza e prendere le “decisioni” più opportune. 
«Se uso un algoritmo di Seldonian per il trattamento del diabete – continua Thomas – posso specificare che un comportamento indesiderato significa glicemia pericolosamente bassa o ipoglicemia. Posso dire alla macchina, “mentre stai cercando di migliorare il controller nella pompa per insulina, non apportare modifiche che aumenterebbero la frequenza dell’ipoglicemia”».